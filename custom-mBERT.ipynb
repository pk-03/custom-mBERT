{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§ó‡•ã‡§µ‡§æ ‡§Æ‡•á‡§Ç ‡§â‡§∞‡•ç‡§´ ‚Äã‚Äã‡§∞‡§ø‡§ï‡•Ä ‡§¨‡§π‡§≤ ‡§¨‡•ç‡§≤‡§° ‡§¨‡§∏‡•ç‡§ü‡§∞‡•ç‡§° ‡§ï‡•Ä ‡§ñ‡•ã‡§ú ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§≤‡§Ç‡§¨‡§æ ‡§î‡§∞ ‡§ä‡§¨ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§™‡•Å‡§∞‡§æ‡§®‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø ‡§®‡•â‡§∏‡•ç‡§ü‡•á‡§≤‡§ú‡§ø‡§ï ‡§π‡•ã‡§®‡§æ ‡§†‡•Ä‡§ï ‡§π‡•à‡•§</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡•á ‡§è‡§ï ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§Æ‡•á‡§Ç, ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§∞‡•ç‡§§‡§æ ‡§Æ‡§®‡•Ä‡§∑ ‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§∞‡§æ‡§µ ‡§Æ‡•É‡§£‡§æ‡§≤ ‡§†‡§æ‡§ï‡•Å‡§∞ ‡§ï‡•ã ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡•Ä‡§µ‡§® ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§â‡§∏‡•á ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§§‡§ï ‡§≠‡•Ä ‡§™‡§π‡•Å‡§Ç‡§ö‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ú‡§ø‡§∏‡•ç‡§Æ‡§´‡§æ‡§∞‡•ã‡§∂‡•Ä ‡§ï‡•á ‡§â‡§∏ ‡§µ‡§ø‡§ö‡§ø‡§§‡•ç‡§∞‡§§‡§æ ‡§∏‡•á ‡§¨‡§æ‡§π‡§∞ ‡§®‡§ø‡§ï‡§≤‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§π‡§Æ‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§Ø‡§π ‡§π‡•à ‡§ï‡§ø ‡§â‡§∏‡§ï‡•Ä ‡§õ‡•ã‡§ü‡•Ä ‡§¨‡§π‡§® ‡§∞‡§ø‡§Ø‡§æ ‡§∞‡§ø‡§Ø‡§æ‡§∏‡§ø‡§∏‡•ã‡§¶‡§ø‡§Ø‡§æ, ‡§ú‡•ã ‡§â‡§∏‡•á ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§∏ ‡§ï‡§æ‡§≤‡•Ä ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§Ü‡§è ‡§π‡•à‡§Ç‡•§ ‡§°‡§æ‡§á‡§∞‡•á‡§ï‡•ç‡§ü‡§∞ ‡§§‡§¨‡§∞‡•á‡§ú‡§º ‡§®‡•Ç‡§∞‡§æ‡§®‡•Ä ‡§®‡•á ‡§¶‡•ã ‡§¨‡§π‡§®‡•ã‡§Ç ‡§ï‡•á ‡§¨‡§π‡§æ‡§®‡•á ‡§Æ‡§æ‡§®‡§µ ‡§Ø‡§æ‡§§‡§æ‡§Ø‡§æ‡§§ ‡§ï‡•Ä ‡§¨‡§¶‡§∏‡•Ç‡§∞‡§§ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§ï‡•Ä ‡§ï‡•ç‡§∞‡•Ç‡§∞ ‡§î‡§∞ ‡§ï‡•ç‡§∞‡•Ç‡§∞ ‡§∏‡§ö‡•ç‡§ö‡§æ‡§à ‡§ï‡•ã ‡§â‡§ú‡§æ‡§ó‡§∞ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§Ü‡§™ ‡§ï‡§ø‡§∏‡§æ‡§® ‡§ï‡•á ‡§ï‡§∞‡•ç‡§ú ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§µ‡§ø‡§ö‡§≤‡§ø‡§§ ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§‡§ï‡§∞‡•ç‡§ú ‡§ï‡§æ ‡§¨‡§∞‡•ç‡§°‡§®‡•§ ‡§´‡§æ‡§∞‡•ç‡§Æ‡§∞ ‡§∂‡§ø‡§µ ‡§Ü‡§¶‡§ø‡§≤ ‡§π‡•Å‡§∏‡•à‡§® ‡§®‡•á ‡§Ö‡§™‡§®‡•Ä ‡§¨‡•á‡§ü‡•Ä ‡§™‡•ç‡§∞‡•Ä‡§§‡§ø ‡§ï‡•ã ‡§Æ‡§æ‡§®‡§µ ‡§§‡§∏‡•ç‡§ï‡§∞‡•Ä ‡§ï‡•á ‡§§‡§π‡§§ ‡§¶‡§æ‡§¶‡§æ ‡§†‡§æ‡§ï‡•Å‡§∞ ‡§Ö‡§®‡•Å‡§™‡§Æ ‡§ñ‡•á‡§∞ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§™‡§®‡•á ‡§ï‡§∞‡•ç‡§ú ‡§ï‡•ã ‡§®‡§ø‡§™‡§ü‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§æ‡§á‡§® ‡§ï‡§ø‡§Ø‡§æ‡•§‡§Ø‡§π‡§æ‡§Å ‡§¨‡§π‡§®, ‡§µ‡§π ‡§≠‡•Ä ‡§è‡§ï ‡§π‡•Ä ‡§ú‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§Ç‡§∏ ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§ï‡•á ‡§Æ‡§æ‡§≤‡§ø‡§ï, ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§ï‡•á ‡§Æ‡§æ‡§≤‡§ø‡§ï, ‡§Æ‡§£‡•ã‡§ú ‡§¨‡§æ‡§ú‡§™‡•á‡§Ø‡•Ä, ‡§≤‡§°‡§º‡§ï‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§≠‡•Ä ‡§°‡§∞‡§æ‡§µ‡§®‡•Ä ‡§∞‡§£‡§®‡•Ä‡§§‡§ø ‡§ï‡•ã ‡§Ö‡§™‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§‡§Ø‡§π ‡§ï‡•Å‡§õ ‡§¨‡§π‡§æ‡§®‡•á ‡§™‡§∞ ‡§¶‡§≤‡§¶‡§≤ ‡§π‡•à‡•§ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§Æ‡•ã‡§°‡§º ‡§§‡§¨ ‡§Ü‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§Ö‡§™‡§®‡•Ä ‡§¨‡§π‡§® ‡§ï‡•ã ‡§™‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ñ‡•Å‡§¶ ‡§ï‡•ã ‡§î‡§∞ ‡§¨‡§π‡§® ‡§ï‡•ã ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§ï‡•Ä ‡§á‡§∏ ‡§≠‡§Ø‡§æ‡§®‡§ï ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§¨‡§æ‡§π‡§∞ ‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à, ‡§á‡§∏‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§™‡§ï‡•ã ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§¶‡•á‡§ñ‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§ ‡§¨‡•â‡§≤‡•Ä‡§µ‡•Å‡§° ‡§Æ‡•á‡§Ç, ‡§Ø‡§π‡§æ‡§Ç ‡§§‡§ï ‚Äã‚Äã‡§ï‡§ø ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•á ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä, ‡§Ø‡§π‡§æ‡§Ç ‡§§‡§ï ‚Äã‚Äã‡§ï‡§ø ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•á ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä,‡§Æ‡§Ç‡§°‡•Ä ‡§¨‡§æ‡§ú‡§º‡§æ‡§∞ ‡§Æ‡•å‡§∏‡§Æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Æ‡•Ä ‡§¨‡•á‡§ó‡§Æ ‡§ú‡§æ‡§® ‡§ú‡•à‡§∏‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ‡•á‡§Ç ‡§¨‡§® ‡§ó‡§è ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡§ï ‡§§‡§¨‡§∞‡•á‡§ú ‡§®‡•Ç‡§∞‡§æ‡§®‡•Ä ‡§ï‡•Ä ‡§¶‡§ø‡§∂‡§æ ‡§∏‡•á ‡§™‡§§‡§æ ‡§ö‡§≤‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¶ ‡§¨‡•ç‡§≤‡•à‡§ï ‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§ë‡§´ ‡§¨‡•â‡§°‡•Ä ‡§ü‡•ç‡§∞‡•á‡§° ‡§ï‡§æ ‡§ó‡§π‡§∞‡§æ ‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•Ä ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ï‡•ã ‡§∏‡§ö‡•ç‡§ö‡•Ä ‡§ò‡§ü‡§®‡§æ‡§ì‡§Ç ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§‡§Æ‡§æ‡§®‡§µ‡§§‡§æ ‡§ï‡•á ‡§Ö‡§Ç‡§ß‡•á‡§∞‡•á ‡§™‡§ï‡•ç‡§∑‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§§‡§∞‡§π ‡§∏‡•á ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•à‡§∏‡•á ‡§≤‡•ã‡§ó ‡§Ö‡§™‡§®‡•á ‡§∏‡•ç‡§µ‡§æ‡§∞‡•ç‡§• ‡§î‡§∞ ‡§Ö‡§∞‡•ç‡§• ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§∂‡•Ä‡§≤ ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ï‡§æ ‡§è‡§ï ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§π‡•à ‡§ú‡•ã ‡§§‡§ø‡§§‡§≤‡•Ä ‡§ï‡•ã ‡§è‡§ï ‡§ú‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡§§‡§æ ‡§π‡•à ‡§ú‡•ã ‡§™‡•Ç‡§∞‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§‡§´‡§ø‡§≤‡•ç‡§Æ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§µ ‡§§‡§∏‡•ç‡§ï‡§∞‡•Ä ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ ‡§≠‡•Ä ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§ï‡§æ‡§∏‡•ç‡§ü‡§ø‡§Ç‡§ó ‡§î‡§∞ ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞ ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à‡§Ç‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§Ö‡§≠‡§ø‡§®‡§Ø ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß ‡§π‡•à‡•§ ‡§™‡§π‡§≤‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§®‡§æ‡§§‡•á, ‡§°‡•á‡§¨‡•ç‡§Ø‡•Ç‡§ü‡•á‡§Ç‡§ü ‡§Æ‡•É‡§£‡§æ‡§≤ ‡§†‡§æ‡§ï‡•Å‡§∞ ‡§®‡•á ‡§á‡§§‡§®‡•Ä ‡§ú‡§¨‡§∞‡§¶‡§∏‡•ç‡§§ ‡§Ö‡§≠‡§ø‡§®‡§Ø ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§Ö‡§∏‡§π‡§®‡•Ä‡§Ø ‡§ï‡§ö‡•ã‡§ü ‡§π‡•ã‡§®‡•á ‡§≤‡§ó‡§§‡•á ‡§π‡•à‡§Ç‡•§‡§ã‡§ö‡§æ ‡§ö‡§°‡•ç‡§π‡§æ ‡§ï‡§æ ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞ ‡§Æ‡§æ‡§®‡§µ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§¶‡§Ø‡•Ä ‡§π‡•à‡•§ ‡§Æ‡§®‡•ç‡§®‡•ã‡§ú‡§º ‡§¨‡§æ‡§ú‡§™‡•á‡§Ø‡•Ä ‡§®‡•á ‡§´‡§ø‡§ú‡§≤ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§¶‡•É‡§¢‡§º‡§§‡§æ ‡§∏‡•á ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§´‡§º‡•ç‡§∞‡•Ä‡§°‡§æ ‡§™‡§ø‡§Ç‡§ü‡•ã ‡§Ö‡§™‡§®‡•Ä ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡§æ ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§°‡§æ‡§≤‡§§‡§æ ‡§π‡•à‡•§ ‡§∏‡§æ‡§à ‡§§‡§Æ‡•ç‡§π‡§Ç‡§ï‡§∞ ‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§∞‡§æ‡§µ ‡§Ö‡§®‡•Å‡§™‡§Æ ‡§ñ‡•á‡§∞ ‡§Ü‡§¶‡§ø‡§≤ ‡§π‡•Å‡§∏‡•à‡§® ‡§®‡•á ‡§Ö‡§™‡§®‡•á ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§‡§°‡•á‡§Æ‡•Ä ‡§Æ‡•Ç‡§∞ ‡§ï‡•ã ‡§∏‡•Å‡§ñ‡§¶ ‡§≤‡§ó‡§§‡§æ ‡§π‡•à‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡§æ ‡§™‡•É‡§∑‡•ç‡§†‡§≠‡•Ç‡§Æ‡§ø ‡§∏‡•ç‡§ï‡•ã‡§∞ ‡§ï‡§æ‡§´‡•Ä ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•á ‡§Æ‡•á‡§≤ ‡§ñ‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§Ø‡§π ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§á‡§∏ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•ã ‡§Ø‡§•‡§æ‡§∞‡•ç‡§•‡§µ‡§æ‡§¶‡•Ä ‡§°‡§æ‡§∞‡•ç‡§ï ‡§´‡§ø‡§≤‡•ç‡§Æ‡•ã‡§Ç ‡§ï‡•á ‡§∂‡•å‡§ï‡•Ä‡§® ‡§¶‡•á‡§ñ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡•§</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§á‡§∏ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§≠‡§æ‡§µ‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü ‡§ï‡•á ‡§¶‡•É‡§∂‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§¶‡§ø‡§Ø‡§æ‡•§</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡§µ‡§ø‡§ï‡•ç‡§∞‡§Æ ‡§õ‡§†‡•á ‡§¶‡§∂‡§ï ‡§ï‡•á ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§ï‡•á ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§î‡§∞ ‡§â‡§¶‡§æ‡§∏ ‡§™‡•ç‡§∞‡•á‡§Æ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ö‡•Å‡§®‡§§‡§æ ‡§π‡•à‡•§</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Reviews  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ‡§ó‡•ã‡§µ‡§æ ‡§Æ‡•á‡§Ç ‡§â‡§∞‡•ç‡§´ ‚Äã‚Äã‡§∞‡§ø‡§ï‡•Ä ‡§¨‡§π‡§≤ ‡§¨‡•ç‡§≤‡§° ‡§¨‡§∏‡•ç‡§ü‡§∞‡•ç‡§° ‡§ï‡•Ä ‡§ñ‡•ã‡§ú ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§≤‡§Ç‡§¨‡§æ ‡§î‡§∞ ‡§ä‡§¨ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ‡§™‡•Å‡§∞‡§æ‡§®‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø ‡§®‡•â‡§∏‡•ç‡§ü‡•á‡§≤‡§ú‡§ø‡§ï ‡§π‡•ã‡§®‡§æ ‡§†‡•Ä‡§ï ‡§π‡•à‡•§   \n",
       "2  ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡•á ‡§è‡§ï ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§Æ‡•á‡§Ç, ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§∞‡•ç‡§§‡§æ ‡§Æ‡§®‡•Ä‡§∑ ‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§∞‡§æ‡§µ ‡§Æ‡•É‡§£‡§æ‡§≤ ‡§†‡§æ‡§ï‡•Å‡§∞ ‡§ï‡•ã ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡•Ä‡§µ‡§® ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§â‡§∏‡•á ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§§‡§ï ‡§≠‡•Ä ‡§™‡§π‡•Å‡§Ç‡§ö‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ú‡§ø‡§∏‡•ç‡§Æ‡§´‡§æ‡§∞‡•ã‡§∂‡•Ä ‡§ï‡•á ‡§â‡§∏ ‡§µ‡§ø‡§ö‡§ø‡§§‡•ç‡§∞‡§§‡§æ ‡§∏‡•á ‡§¨‡§æ‡§π‡§∞ ‡§®‡§ø‡§ï‡§≤‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§π‡§Æ‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§Ø‡§π ‡§π‡•à ‡§ï‡§ø ‡§â‡§∏‡§ï‡•Ä ‡§õ‡•ã‡§ü‡•Ä ‡§¨‡§π‡§® ‡§∞‡§ø‡§Ø‡§æ ‡§∞‡§ø‡§Ø‡§æ‡§∏‡§ø‡§∏‡•ã‡§¶‡§ø‡§Ø‡§æ, ‡§ú‡•ã ‡§â‡§∏‡•á ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§∏ ‡§ï‡§æ‡§≤‡•Ä ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§Ü‡§è ‡§π‡•à‡§Ç‡•§ ‡§°‡§æ‡§á‡§∞‡•á‡§ï‡•ç‡§ü‡§∞ ‡§§‡§¨‡§∞‡•á‡§ú‡§º ‡§®‡•Ç‡§∞‡§æ‡§®‡•Ä ‡§®‡•á ‡§¶‡•ã ‡§¨‡§π‡§®‡•ã‡§Ç ‡§ï‡•á ‡§¨‡§π‡§æ‡§®‡•á ‡§Æ‡§æ‡§®‡§µ ‡§Ø‡§æ‡§§‡§æ‡§Ø‡§æ‡§§ ‡§ï‡•Ä ‡§¨‡§¶‡§∏‡•Ç‡§∞‡§§ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§ï‡•Ä ‡§ï‡•ç‡§∞‡•Ç‡§∞ ‡§î‡§∞ ‡§ï‡•ç‡§∞‡•Ç‡§∞ ‡§∏‡§ö‡•ç‡§ö‡§æ‡§à ‡§ï‡•ã ‡§â‡§ú‡§æ‡§ó‡§∞ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§Ü‡§™ ‡§ï‡§ø‡§∏‡§æ‡§® ‡§ï‡•á ‡§ï‡§∞‡•ç‡§ú ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§µ‡§ø‡§ö‡§≤‡§ø‡§§ ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§‡§ï‡§∞‡•ç‡§ú ‡§ï‡§æ ‡§¨‡§∞‡•ç‡§°‡§®‡•§ ‡§´‡§æ‡§∞‡•ç‡§Æ‡§∞ ‡§∂‡§ø‡§µ ‡§Ü‡§¶‡§ø‡§≤ ‡§π‡•Å‡§∏‡•à‡§® ‡§®‡•á ‡§Ö‡§™‡§®‡•Ä ‡§¨‡•á‡§ü‡•Ä ‡§™‡•ç‡§∞‡•Ä‡§§‡§ø ‡§ï‡•ã ‡§Æ‡§æ‡§®‡§µ ‡§§‡§∏‡•ç‡§ï‡§∞‡•Ä ‡§ï‡•á ‡§§‡§π‡§§ ‡§¶‡§æ‡§¶‡§æ ‡§†‡§æ‡§ï‡•Å‡§∞ ‡§Ö‡§®‡•Å‡§™‡§Æ ‡§ñ‡•á‡§∞ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§™‡§®‡•á ‡§ï‡§∞‡•ç‡§ú ‡§ï‡•ã ‡§®‡§ø‡§™‡§ü‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§æ‡§á‡§® ‡§ï‡§ø‡§Ø‡§æ‡•§‡§Ø‡§π‡§æ‡§Å ‡§¨‡§π‡§®, ‡§µ‡§π ‡§≠‡•Ä ‡§è‡§ï ‡§π‡•Ä ‡§ú‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§Ç‡§∏ ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§ï‡•á ‡§Æ‡§æ‡§≤‡§ø‡§ï, ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§ï‡•á ‡§Æ‡§æ‡§≤‡§ø‡§ï, ‡§Æ‡§£‡•ã‡§ú ‡§¨‡§æ‡§ú‡§™‡•á‡§Ø‡•Ä, ‡§≤‡§°‡§º‡§ï‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§≠‡•Ä ‡§°‡§∞‡§æ‡§µ‡§®‡•Ä ‡§∞‡§£‡§®‡•Ä‡§§‡§ø ‡§ï‡•ã ‡§Ö‡§™‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§‡§Ø‡§π ‡§ï‡•Å‡§õ ‡§¨‡§π‡§æ‡§®‡•á ‡§™‡§∞ ‡§¶‡§≤‡§¶‡§≤ ‡§π‡•à‡•§ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§Æ‡•ã‡§°‡§º ‡§§‡§¨ ‡§Ü‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§Ö‡§™‡§®‡•Ä ‡§¨‡§π‡§® ‡§ï‡•ã ‡§™‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ñ‡•Å‡§¶ ‡§ï‡•ã ‡§î‡§∞ ‡§¨‡§π‡§® ‡§ï‡•ã ‡§µ‡•á‡§∂‡•ç‡§Ø‡§æ‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§ï‡•Ä ‡§á‡§∏ ‡§≠‡§Ø‡§æ‡§®‡§ï ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§¨‡§æ‡§π‡§∞ ‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à, ‡§á‡§∏‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§™‡§ï‡•ã ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§¶‡•á‡§ñ‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§ ‡§¨‡•â‡§≤‡•Ä‡§µ‡•Å‡§° ‡§Æ‡•á‡§Ç, ‡§Ø‡§π‡§æ‡§Ç ‡§§‡§ï ‚Äã‚Äã‡§ï‡§ø ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•á ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä, ‡§Ø‡§π‡§æ‡§Ç ‡§§‡§ï ‚Äã‚Äã‡§ï‡§ø ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•á ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä,‡§Æ‡§Ç‡§°‡•Ä ‡§¨‡§æ‡§ú‡§º‡§æ‡§∞ ‡§Æ‡•å‡§∏‡§Æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Æ‡•Ä ‡§¨‡•á‡§ó‡§Æ ‡§ú‡§æ‡§® ‡§ú‡•à‡§∏‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ‡•á‡§Ç ‡§¨‡§® ‡§ó‡§è ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡§ï ‡§§‡§¨‡§∞‡•á‡§ú ‡§®‡•Ç‡§∞‡§æ‡§®‡•Ä ‡§ï‡•Ä ‡§¶‡§ø‡§∂‡§æ ‡§∏‡•á ‡§™‡§§‡§æ ‡§ö‡§≤‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¶ ‡§¨‡•ç‡§≤‡•à‡§ï ‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§ë‡§´ ‡§¨‡•â‡§°‡•Ä ‡§ü‡•ç‡§∞‡•á‡§° ‡§ï‡§æ ‡§ó‡§π‡§∞‡§æ ‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•Ä ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ï‡•ã ‡§∏‡§ö‡•ç‡§ö‡•Ä ‡§ò‡§ü‡§®‡§æ‡§ì‡§Ç ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§‡§Æ‡§æ‡§®‡§µ‡§§‡§æ ‡§ï‡•á ‡§Ö‡§Ç‡§ß‡•á‡§∞‡•á ‡§™‡§ï‡•ç‡§∑‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§§‡§∞‡§π ‡§∏‡•á ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•à‡§∏‡•á ‡§≤‡•ã‡§ó ‡§Ö‡§™‡§®‡•á ‡§∏‡•ç‡§µ‡§æ‡§∞‡•ç‡§• ‡§î‡§∞ ‡§Ö‡§∞‡•ç‡§• ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§∂‡•Ä‡§≤ ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ï‡§æ ‡§è‡§ï ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§π‡•à ‡§ú‡•ã ‡§§‡§ø‡§§‡§≤‡•Ä ‡§ï‡•ã ‡§è‡§ï ‡§ú‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡§§‡§æ ‡§π‡•à ‡§ú‡•ã ‡§™‡•Ç‡§∞‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§‡§´‡§ø‡§≤‡•ç‡§Æ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§µ ‡§§‡§∏‡•ç‡§ï‡§∞‡•Ä ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ ‡§≠‡•Ä ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§ï‡§æ‡§∏‡•ç‡§ü‡§ø‡§Ç‡§ó ‡§î‡§∞ ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞ ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à‡§Ç‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§Ö‡§≠‡§ø‡§®‡§Ø ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß ‡§π‡•à‡•§ ‡§™‡§π‡§≤‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§®‡§æ‡§§‡•á, ‡§°‡•á‡§¨‡•ç‡§Ø‡•Ç‡§ü‡•á‡§Ç‡§ü ‡§Æ‡•É‡§£‡§æ‡§≤ ‡§†‡§æ‡§ï‡•Å‡§∞ ‡§®‡•á ‡§á‡§§‡§®‡•Ä ‡§ú‡§¨‡§∞‡§¶‡§∏‡•ç‡§§ ‡§Ö‡§≠‡§ø‡§®‡§Ø ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§Ö‡§∏‡§π‡§®‡•Ä‡§Ø ‡§ï‡§ö‡•ã‡§ü ‡§π‡•ã‡§®‡•á ‡§≤‡§ó‡§§‡•á ‡§π‡•à‡§Ç‡•§‡§ã‡§ö‡§æ ‡§ö‡§°‡•ç‡§π‡§æ ‡§ï‡§æ ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞ ‡§Æ‡§æ‡§®‡§µ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§¶‡§Ø‡•Ä ‡§π‡•à‡•§ ‡§Æ‡§®‡•ç‡§®‡•ã‡§ú‡§º ‡§¨‡§æ‡§ú‡§™‡•á‡§Ø‡•Ä ‡§®‡•á ‡§´‡§ø‡§ú‡§≤ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§¶‡•É‡§¢‡§º‡§§‡§æ ‡§∏‡•á ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§´‡§º‡•ç‡§∞‡•Ä‡§°‡§æ ‡§™‡§ø‡§Ç‡§ü‡•ã ‡§Ö‡§™‡§®‡•Ä ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡§æ ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§°‡§æ‡§≤‡§§‡§æ ‡§π‡•à‡•§ ‡§∏‡§æ‡§à ‡§§‡§Æ‡•ç‡§π‡§Ç‡§ï‡§∞ ‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§∞‡§æ‡§µ ‡§Ö‡§®‡•Å‡§™‡§Æ ‡§ñ‡•á‡§∞ ‡§Ü‡§¶‡§ø‡§≤ ‡§π‡•Å‡§∏‡•à‡§® ‡§®‡•á ‡§Ö‡§™‡§®‡•á ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§‡§°‡•á‡§Æ‡•Ä ‡§Æ‡•Ç‡§∞ ‡§ï‡•ã ‡§∏‡•Å‡§ñ‡§¶ ‡§≤‡§ó‡§§‡§æ ‡§π‡•à‡•§ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡§æ ‡§™‡•É‡§∑‡•ç‡§†‡§≠‡•Ç‡§Æ‡§ø ‡§∏‡•ç‡§ï‡•ã‡§∞ ‡§ï‡§æ‡§´‡•Ä ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•á ‡§Æ‡•á‡§≤ ‡§ñ‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§Ø‡§π ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§á‡§∏ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•ã ‡§Ø‡§•‡§æ‡§∞‡•ç‡§•‡§µ‡§æ‡§¶‡•Ä ‡§°‡§æ‡§∞‡•ç‡§ï ‡§´‡§ø‡§≤‡•ç‡§Æ‡•ã‡§Ç ‡§ï‡•á ‡§∂‡•å‡§ï‡•Ä‡§® ‡§¶‡•á‡§ñ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡•§   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§á‡§∏ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§≠‡§æ‡§µ‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü ‡§ï‡•á ‡§¶‡•É‡§∂‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§¶‡§ø‡§Ø‡§æ‡•§   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ‡§µ‡§ø‡§ï‡•ç‡§∞‡§Æ ‡§õ‡§†‡•á ‡§¶‡§∂‡§ï ‡§ï‡•á ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§ï‡•á ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§î‡§∞ ‡§â‡§¶‡§æ‡§∏ ‡§™‡•ç‡§∞‡•á‡§Æ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ö‡•Å‡§®‡§§‡§æ ‡§π‡•à‡•§   \n",
       "\n",
       "     labels  \n",
       "0  negative  \n",
       "1   neutral  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../augmented_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reviews    0\n",
       "labels     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import BertTokenizer\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2852: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lenovo/.local/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14905' max='16260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14905/16260 2:01:56 < 11:05, 2.04 it/s, Epoch 55/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.026144</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.414747</td>\n",
       "      <td>0.502317</td>\n",
       "      <td>0.475806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.045700</td>\n",
       "      <td>0.981093</td>\n",
       "      <td>0.493952</td>\n",
       "      <td>0.496627</td>\n",
       "      <td>0.504465</td>\n",
       "      <td>0.493952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.045700</td>\n",
       "      <td>0.953522</td>\n",
       "      <td>0.549059</td>\n",
       "      <td>0.542783</td>\n",
       "      <td>0.548658</td>\n",
       "      <td>0.549059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>0.917337</td>\n",
       "      <td>0.577285</td>\n",
       "      <td>0.573727</td>\n",
       "      <td>0.577723</td>\n",
       "      <td>0.577285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>0.877550</td>\n",
       "      <td>0.622312</td>\n",
       "      <td>0.623180</td>\n",
       "      <td>0.627116</td>\n",
       "      <td>0.622312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.828420</td>\n",
       "      <td>0.647177</td>\n",
       "      <td>0.648154</td>\n",
       "      <td>0.653457</td>\n",
       "      <td>0.647177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.841001</td>\n",
       "      <td>0.669355</td>\n",
       "      <td>0.670244</td>\n",
       "      <td>0.674387</td>\n",
       "      <td>0.669355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>1.006898</td>\n",
       "      <td>0.680108</td>\n",
       "      <td>0.668979</td>\n",
       "      <td>0.702852</td>\n",
       "      <td>0.680108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.705645</td>\n",
       "      <td>0.706499</td>\n",
       "      <td>0.711198</td>\n",
       "      <td>0.705645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>1.041849</td>\n",
       "      <td>0.698253</td>\n",
       "      <td>0.700324</td>\n",
       "      <td>0.709436</td>\n",
       "      <td>0.698253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>1.184589</td>\n",
       "      <td>0.699597</td>\n",
       "      <td>0.699112</td>\n",
       "      <td>0.704258</td>\n",
       "      <td>0.699597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>1.333228</td>\n",
       "      <td>0.713710</td>\n",
       "      <td>0.714466</td>\n",
       "      <td>0.726202</td>\n",
       "      <td>0.713710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>1.314013</td>\n",
       "      <td>0.726478</td>\n",
       "      <td>0.724738</td>\n",
       "      <td>0.733635</td>\n",
       "      <td>0.726478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>1.394729</td>\n",
       "      <td>0.715054</td>\n",
       "      <td>0.716488</td>\n",
       "      <td>0.721918</td>\n",
       "      <td>0.715054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>1.362279</td>\n",
       "      <td>0.728495</td>\n",
       "      <td>0.729899</td>\n",
       "      <td>0.733981</td>\n",
       "      <td>0.728495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>1.527108</td>\n",
       "      <td>0.727151</td>\n",
       "      <td>0.728927</td>\n",
       "      <td>0.738386</td>\n",
       "      <td>0.727151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>1.526903</td>\n",
       "      <td>0.739919</td>\n",
       "      <td>0.740471</td>\n",
       "      <td>0.742254</td>\n",
       "      <td>0.739919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>1.665248</td>\n",
       "      <td>0.729839</td>\n",
       "      <td>0.728746</td>\n",
       "      <td>0.729138</td>\n",
       "      <td>0.729839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>1.629902</td>\n",
       "      <td>0.739247</td>\n",
       "      <td>0.740316</td>\n",
       "      <td>0.750383</td>\n",
       "      <td>0.739247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>1.793167</td>\n",
       "      <td>0.734543</td>\n",
       "      <td>0.735872</td>\n",
       "      <td>0.741123</td>\n",
       "      <td>0.734543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>1.718989</td>\n",
       "      <td>0.760081</td>\n",
       "      <td>0.757644</td>\n",
       "      <td>0.760499</td>\n",
       "      <td>0.760081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>1.724344</td>\n",
       "      <td>0.750672</td>\n",
       "      <td>0.750127</td>\n",
       "      <td>0.751029</td>\n",
       "      <td>0.750672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>1.816399</td>\n",
       "      <td>0.747984</td>\n",
       "      <td>0.746850</td>\n",
       "      <td>0.749836</td>\n",
       "      <td>0.747984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>1.719334</td>\n",
       "      <td>0.746640</td>\n",
       "      <td>0.746592</td>\n",
       "      <td>0.746614</td>\n",
       "      <td>0.746640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>1.926192</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.743286</td>\n",
       "      <td>0.747039</td>\n",
       "      <td>0.742608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>1.882395</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>0.753641</td>\n",
       "      <td>0.756613</td>\n",
       "      <td>0.753360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>1.802958</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.757768</td>\n",
       "      <td>0.758054</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.835389</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>0.754511</td>\n",
       "      <td>0.755419</td>\n",
       "      <td>0.754032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.987240</td>\n",
       "      <td>0.745968</td>\n",
       "      <td>0.745972</td>\n",
       "      <td>0.746387</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>1.828523</td>\n",
       "      <td>0.762097</td>\n",
       "      <td>0.762285</td>\n",
       "      <td>0.763294</td>\n",
       "      <td>0.762097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>1.942820</td>\n",
       "      <td>0.756720</td>\n",
       "      <td>0.756877</td>\n",
       "      <td>0.757107</td>\n",
       "      <td>0.756720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>1.953308</td>\n",
       "      <td>0.759409</td>\n",
       "      <td>0.759795</td>\n",
       "      <td>0.762922</td>\n",
       "      <td>0.759409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>1.976434</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>0.754522</td>\n",
       "      <td>0.758443</td>\n",
       "      <td>0.753360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.889008</td>\n",
       "      <td>0.755376</td>\n",
       "      <td>0.753680</td>\n",
       "      <td>0.755387</td>\n",
       "      <td>0.755376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.938413</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.757681</td>\n",
       "      <td>0.764661</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>1.848421</td>\n",
       "      <td>0.775538</td>\n",
       "      <td>0.775303</td>\n",
       "      <td>0.775412</td>\n",
       "      <td>0.775538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.941579</td>\n",
       "      <td>0.756720</td>\n",
       "      <td>0.756076</td>\n",
       "      <td>0.758608</td>\n",
       "      <td>0.756720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.965487</td>\n",
       "      <td>0.754704</td>\n",
       "      <td>0.755697</td>\n",
       "      <td>0.760225</td>\n",
       "      <td>0.754704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>1.900889</td>\n",
       "      <td>0.765457</td>\n",
       "      <td>0.766787</td>\n",
       "      <td>0.770716</td>\n",
       "      <td>0.765457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>1.892345</td>\n",
       "      <td>0.774866</td>\n",
       "      <td>0.774145</td>\n",
       "      <td>0.774427</td>\n",
       "      <td>0.774866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>1.911134</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.772372</td>\n",
       "      <td>0.776085</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>1.937653</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>0.775275</td>\n",
       "      <td>0.781443</td>\n",
       "      <td>0.776882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.975113</td>\n",
       "      <td>0.769489</td>\n",
       "      <td>0.770021</td>\n",
       "      <td>0.774236</td>\n",
       "      <td>0.769489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.907779</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.769443</td>\n",
       "      <td>0.770558</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>1.910553</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.783171</td>\n",
       "      <td>0.783338</td>\n",
       "      <td>0.783602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>1.918241</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.777584</td>\n",
       "      <td>0.780779</td>\n",
       "      <td>0.778898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>1.954546</td>\n",
       "      <td>0.771505</td>\n",
       "      <td>0.770335</td>\n",
       "      <td>0.771234</td>\n",
       "      <td>0.771505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.966388</td>\n",
       "      <td>0.768145</td>\n",
       "      <td>0.767759</td>\n",
       "      <td>0.767550</td>\n",
       "      <td>0.768145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.953016</td>\n",
       "      <td>0.773522</td>\n",
       "      <td>0.770881</td>\n",
       "      <td>0.777310</td>\n",
       "      <td>0.773522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>2.105201</td>\n",
       "      <td>0.755376</td>\n",
       "      <td>0.756606</td>\n",
       "      <td>0.760316</td>\n",
       "      <td>0.755376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1.933426</td>\n",
       "      <td>0.775538</td>\n",
       "      <td>0.773286</td>\n",
       "      <td>0.776074</td>\n",
       "      <td>0.775538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.924369</td>\n",
       "      <td>0.782930</td>\n",
       "      <td>0.782789</td>\n",
       "      <td>0.784981</td>\n",
       "      <td>0.782930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.907105</td>\n",
       "      <td>0.782930</td>\n",
       "      <td>0.782472</td>\n",
       "      <td>0.782295</td>\n",
       "      <td>0.782930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.933050</td>\n",
       "      <td>0.780242</td>\n",
       "      <td>0.780008</td>\n",
       "      <td>0.780053</td>\n",
       "      <td>0.780242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.973383</td>\n",
       "      <td>0.773522</td>\n",
       "      <td>0.773624</td>\n",
       "      <td>0.774083</td>\n",
       "      <td>0.773522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14905, training_loss=0.16941646229137075, metrics={'train_runtime': 7316.9973, 'train_samples_per_second': 48.807, 'train_steps_per_second': 2.222, 'total_flos': 2.153320210685952e+16, 'train_loss': 0.16941646229137075, 'epoch': 55.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset (assuming CSV format with columns 'text' and 'label')\n",
    "data = df.copy()\n",
    "texts = data['Reviews'].tolist()\n",
    "labels = data['labels'].tolist()\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the mBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def polyglot_tokenize(text):\n",
    "    polyglot_text = Text(text, hint_language_code='hi')  # Tokenize with Polyglot\n",
    "    return polyglot_text.words  # Return tokens as a list\n",
    "\n",
    "def custom_tokenize(texts, tokenizer, max_length=128):\n",
    "    all_input_ids = []\n",
    "    for text in texts:\n",
    "        # Step 1: Tokenize using Polyglot\n",
    "        polyglot_tokens = polyglot_tokenize(text)\n",
    "\n",
    "        # Step 2: Map Polyglot tokens to mBERT vocabulary\n",
    "        input_ids = []\n",
    "        for token in polyglot_tokens:\n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            if token_id is None:  # Handle tokens not in mBERT's vocabulary\n",
    "                token_id = tokenizer.unk_token_id  # Use [UNK] token ID\n",
    "            input_ids.append(token_id)\n",
    "\n",
    "        # Step 3: Truncate sequences if necessary\n",
    "        if len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "\n",
    "    # Step 4: Pad sequences to the maximum length\n",
    "    encoded = tokenizer.pad(\n",
    "        {\"input_ids\": all_input_ids},\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded\n",
    "\n",
    "\n",
    "train_encodings = custom_tokenize(train_texts, tokenizer)\n",
    "test_encodings = custom_tokenize(test_texts, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2} # Create a mapping\n",
    "train_labels = [label_mapping[label] for label in train_labels] # Apply mapping\n",
    "test_labels = [label_mapping[label] for label in test_labels]\n",
    "\n",
    "\n",
    "# Create Dataset objects for Trainer\n",
    "class HindiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = HindiTextDataset(train_encodings, train_labels)\n",
    "test_dataset = HindiTextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Load the mBERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(set(labels)))\n",
    "model.to(device)\n",
    "\n",
    "# Define multiple metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Set up Trainer with early stopping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=22,\n",
    "    per_device_eval_batch_size=22,\n",
    "    num_train_epochs=60,  # Max epochs\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=2,  # Limit saved checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,  # Evaluate on test dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a9009bde3882>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation Results: {'eval_loss': 1.1041765213012695, 'eval_accuracy': 0.8649193548387096, 'eval_f1': 0.8646433553096033, 'eval_precision': 0.8649455810283394, 'eval_recall': 0.8649193548387096, 'eval_runtime': 11.009, 'eval_samples_per_second': 135.162, 'eval_steps_per_second': 6.177, 'epoch': 30.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Evaluation Results:\", test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
