{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>गोवा में उर्फ ​​रिकी बहल ब्लड बस्टर्ड की खोज का संदर्भ लंबा और ऊब गया है।</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>पुरानी फिल्म के प्रति नॉस्टेलजिक होना ठीक है।</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>सोनिया के प्यार के एक दृश्य में, सामाजिक कार्यकर्ता मनीष राजकुमार राव मृणाल ठाकुर को बचाने के लिए जीवन देता है और उसे बचाने के लिए वेश्यालय तक भी पहुंचता है, लेकिन सोनिया जिस्मफारोशी के उस विचित्रता से बाहर निकलने के लिए सहमत नहीं है। इसका कारण यह है कि उसकी छोटी बहन रिया रियासिसोदिया, जो उसे बचाने के लिए इस काली दुनिया में आए हैं। डाइरेक्टर तबरेज़ नूरानी ने दो बहनों के बहाने मानव यातायात की बदसूरत दुनिया की क्रूर और क्रूर सच्चाई को उजागर किया है, जो आप किसान के कर्ज को देखकर विचलित हो जाते हैं।कर्ज का बर्डन। फार्मर शिव आदिल हुसैन ने अपनी बेटी प्रीति को मानव तस्करी के तहत दादा ठाकुर अनुपम खेर के साथ अपने कर्ज को निपटाने के लिए साइन किया।यहाँ बहन, वह भी एक ही जाल में फंस जाती है। वेश्यालय के मालिक, वेश्यालय के मालिक, मणोज बाजपेयी, लड़कियों को पेशेवर रूप से प्राप्त करने के लिए सभी डरावनी रणनीति को अपनाते हैं।यह कुछ बहाने पर दलदल है। कहानी में मोड़ तब आता है जब सोनिया अपनी बहन को पाता है। यह सोनिया खुद को और बहन को वेश्यावृत्ति की इस भयानक दुनिया से बाहर कर देता है, इसके लिए आपको फिल्म देखना होगा। बॉलीवुड में, यहां तक ​​कि शरीर के व्यापार में भी, यहां तक ​​कि शरीर के व्यापार में भी,मंडी बाज़ार मौसम लक्ष्मी बेगम जान जैसी फिल्में बन गए हैं, लेकिन निर्देशक तबरेज नूरानी की दिशा से पता चलता है कि उन्होंने द ब्लैक वर्ल्ड ऑफ बॉडी ट्रेड का गहरा अध्ययन किया है। फिल्म की कहानी को सच्ची घटनाओं पर आधारित कहा जाता है।मानवता के अंधेरे पक्षों को बहुत अच्छी तरह से दिखाया है कि कैसे लोग अपने स्वार्थ और अर्थ के लिए असंवेदनशील हो जाते हैं। सोनिया का एक दृश्य है जो तितली को एक जार में रखता है जो पूरी फिल्म की प्रकृति का वर्णन करता है।फिल्म में विदेशों में मानव तस्करी का विस्तार भी होना चाहिए। फिल्म के कास्टिंग और चरित्र दिलचस्प हैं। फिल्म अभिनय में बहुत समृद्ध है। पहली फिल्म होने के नाते, डेब्यूटेंट मृणाल ठाकुर ने इतनी जबरदस्त अभिनय किया है कि आप असहनीय कचोट होने लगते हैं।ऋचा चड्हा का चरित्र मानव के रूप में निर्दयी है। मन्नोज़ बाजपेयी ने फिजल के रूप में दृढ़ता से प्रदर्शन किया है। फ़्रीडा पिंटो अपनी भूमिका में प्रभाव डालता है। साई तम्हंकर राजकुमार राव अनुपम खेर आदिल हुसैन ने अपने चरित्रों के अनुसार बहुत अच्छा प्रदर्शन किया है।डेमी मूर को सुखद लगता है। फिल्म का पृष्ठभूमि स्कोर काफी दिलचस्प है जो कहानी से मेल खाती है। क्यों यह फिल्म इस फिल्म को यथार्थवादी डार्क फिल्मों के शौकीन देख सकती है।</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>उन्होंने इस फिल्म के भावनात्मक विस्फोट के दृश्यों को अधिक नहीं दिया।</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>विक्रम छठे दशक के बंगाल के संदर्भ में एक दिलचस्प और उदास प्रेम कहानी चुनता है।</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Reviews  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     गोवा में उर्फ ​​रिकी बहल ब्लड बस्टर्ड की खोज का संदर्भ लंबा और ऊब गया है।   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 पुरानी फिल्म के प्रति नॉस्टेलजिक होना ठीक है।   \n",
       "2  सोनिया के प्यार के एक दृश्य में, सामाजिक कार्यकर्ता मनीष राजकुमार राव मृणाल ठाकुर को बचाने के लिए जीवन देता है और उसे बचाने के लिए वेश्यालय तक भी पहुंचता है, लेकिन सोनिया जिस्मफारोशी के उस विचित्रता से बाहर निकलने के लिए सहमत नहीं है। इसका कारण यह है कि उसकी छोटी बहन रिया रियासिसोदिया, जो उसे बचाने के लिए इस काली दुनिया में आए हैं। डाइरेक्टर तबरेज़ नूरानी ने दो बहनों के बहाने मानव यातायात की बदसूरत दुनिया की क्रूर और क्रूर सच्चाई को उजागर किया है, जो आप किसान के कर्ज को देखकर विचलित हो जाते हैं।कर्ज का बर्डन। फार्मर शिव आदिल हुसैन ने अपनी बेटी प्रीति को मानव तस्करी के तहत दादा ठाकुर अनुपम खेर के साथ अपने कर्ज को निपटाने के लिए साइन किया।यहाँ बहन, वह भी एक ही जाल में फंस जाती है। वेश्यालय के मालिक, वेश्यालय के मालिक, मणोज बाजपेयी, लड़कियों को पेशेवर रूप से प्राप्त करने के लिए सभी डरावनी रणनीति को अपनाते हैं।यह कुछ बहाने पर दलदल है। कहानी में मोड़ तब आता है जब सोनिया अपनी बहन को पाता है। यह सोनिया खुद को और बहन को वेश्यावृत्ति की इस भयानक दुनिया से बाहर कर देता है, इसके लिए आपको फिल्म देखना होगा। बॉलीवुड में, यहां तक ​​कि शरीर के व्यापार में भी, यहां तक ​​कि शरीर के व्यापार में भी,मंडी बाज़ार मौसम लक्ष्मी बेगम जान जैसी फिल्में बन गए हैं, लेकिन निर्देशक तबरेज नूरानी की दिशा से पता चलता है कि उन्होंने द ब्लैक वर्ल्ड ऑफ बॉडी ट्रेड का गहरा अध्ययन किया है। फिल्म की कहानी को सच्ची घटनाओं पर आधारित कहा जाता है।मानवता के अंधेरे पक्षों को बहुत अच्छी तरह से दिखाया है कि कैसे लोग अपने स्वार्थ और अर्थ के लिए असंवेदनशील हो जाते हैं। सोनिया का एक दृश्य है जो तितली को एक जार में रखता है जो पूरी फिल्म की प्रकृति का वर्णन करता है।फिल्म में विदेशों में मानव तस्करी का विस्तार भी होना चाहिए। फिल्म के कास्टिंग और चरित्र दिलचस्प हैं। फिल्म अभिनय में बहुत समृद्ध है। पहली फिल्म होने के नाते, डेब्यूटेंट मृणाल ठाकुर ने इतनी जबरदस्त अभिनय किया है कि आप असहनीय कचोट होने लगते हैं।ऋचा चड्हा का चरित्र मानव के रूप में निर्दयी है। मन्नोज़ बाजपेयी ने फिजल के रूप में दृढ़ता से प्रदर्शन किया है। फ़्रीडा पिंटो अपनी भूमिका में प्रभाव डालता है। साई तम्हंकर राजकुमार राव अनुपम खेर आदिल हुसैन ने अपने चरित्रों के अनुसार बहुत अच्छा प्रदर्शन किया है।डेमी मूर को सुखद लगता है। फिल्म का पृष्ठभूमि स्कोर काफी दिलचस्प है जो कहानी से मेल खाती है। क्यों यह फिल्म इस फिल्म को यथार्थवादी डार्क फिल्मों के शौकीन देख सकती है।   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          उन्होंने इस फिल्म के भावनात्मक विस्फोट के दृश्यों को अधिक नहीं दिया।   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                विक्रम छठे दशक के बंगाल के संदर्भ में एक दिलचस्प और उदास प्रेम कहानी चुनता है।   \n",
       "\n",
       "     labels  \n",
       "0  negative  \n",
       "1   neutral  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../augmented_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reviews    0\n",
       "labels     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import BertTokenizer\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2852: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lenovo/.local/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14905' max='16260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14905/16260 2:01:56 < 11:05, 2.04 it/s, Epoch 55/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.026144</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.414747</td>\n",
       "      <td>0.502317</td>\n",
       "      <td>0.475806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.045700</td>\n",
       "      <td>0.981093</td>\n",
       "      <td>0.493952</td>\n",
       "      <td>0.496627</td>\n",
       "      <td>0.504465</td>\n",
       "      <td>0.493952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.045700</td>\n",
       "      <td>0.953522</td>\n",
       "      <td>0.549059</td>\n",
       "      <td>0.542783</td>\n",
       "      <td>0.548658</td>\n",
       "      <td>0.549059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>0.917337</td>\n",
       "      <td>0.577285</td>\n",
       "      <td>0.573727</td>\n",
       "      <td>0.577723</td>\n",
       "      <td>0.577285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>0.877550</td>\n",
       "      <td>0.622312</td>\n",
       "      <td>0.623180</td>\n",
       "      <td>0.627116</td>\n",
       "      <td>0.622312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.828420</td>\n",
       "      <td>0.647177</td>\n",
       "      <td>0.648154</td>\n",
       "      <td>0.653457</td>\n",
       "      <td>0.647177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.841001</td>\n",
       "      <td>0.669355</td>\n",
       "      <td>0.670244</td>\n",
       "      <td>0.674387</td>\n",
       "      <td>0.669355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>1.006898</td>\n",
       "      <td>0.680108</td>\n",
       "      <td>0.668979</td>\n",
       "      <td>0.702852</td>\n",
       "      <td>0.680108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.705645</td>\n",
       "      <td>0.706499</td>\n",
       "      <td>0.711198</td>\n",
       "      <td>0.705645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>1.041849</td>\n",
       "      <td>0.698253</td>\n",
       "      <td>0.700324</td>\n",
       "      <td>0.709436</td>\n",
       "      <td>0.698253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>1.184589</td>\n",
       "      <td>0.699597</td>\n",
       "      <td>0.699112</td>\n",
       "      <td>0.704258</td>\n",
       "      <td>0.699597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>1.333228</td>\n",
       "      <td>0.713710</td>\n",
       "      <td>0.714466</td>\n",
       "      <td>0.726202</td>\n",
       "      <td>0.713710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>1.314013</td>\n",
       "      <td>0.726478</td>\n",
       "      <td>0.724738</td>\n",
       "      <td>0.733635</td>\n",
       "      <td>0.726478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>1.394729</td>\n",
       "      <td>0.715054</td>\n",
       "      <td>0.716488</td>\n",
       "      <td>0.721918</td>\n",
       "      <td>0.715054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>1.362279</td>\n",
       "      <td>0.728495</td>\n",
       "      <td>0.729899</td>\n",
       "      <td>0.733981</td>\n",
       "      <td>0.728495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>1.527108</td>\n",
       "      <td>0.727151</td>\n",
       "      <td>0.728927</td>\n",
       "      <td>0.738386</td>\n",
       "      <td>0.727151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>1.526903</td>\n",
       "      <td>0.739919</td>\n",
       "      <td>0.740471</td>\n",
       "      <td>0.742254</td>\n",
       "      <td>0.739919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>1.665248</td>\n",
       "      <td>0.729839</td>\n",
       "      <td>0.728746</td>\n",
       "      <td>0.729138</td>\n",
       "      <td>0.729839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>1.629902</td>\n",
       "      <td>0.739247</td>\n",
       "      <td>0.740316</td>\n",
       "      <td>0.750383</td>\n",
       "      <td>0.739247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>1.793167</td>\n",
       "      <td>0.734543</td>\n",
       "      <td>0.735872</td>\n",
       "      <td>0.741123</td>\n",
       "      <td>0.734543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>1.718989</td>\n",
       "      <td>0.760081</td>\n",
       "      <td>0.757644</td>\n",
       "      <td>0.760499</td>\n",
       "      <td>0.760081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>1.724344</td>\n",
       "      <td>0.750672</td>\n",
       "      <td>0.750127</td>\n",
       "      <td>0.751029</td>\n",
       "      <td>0.750672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>1.816399</td>\n",
       "      <td>0.747984</td>\n",
       "      <td>0.746850</td>\n",
       "      <td>0.749836</td>\n",
       "      <td>0.747984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>1.719334</td>\n",
       "      <td>0.746640</td>\n",
       "      <td>0.746592</td>\n",
       "      <td>0.746614</td>\n",
       "      <td>0.746640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>1.926192</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.743286</td>\n",
       "      <td>0.747039</td>\n",
       "      <td>0.742608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>1.882395</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>0.753641</td>\n",
       "      <td>0.756613</td>\n",
       "      <td>0.753360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>1.802958</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.757768</td>\n",
       "      <td>0.758054</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.835389</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>0.754511</td>\n",
       "      <td>0.755419</td>\n",
       "      <td>0.754032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.987240</td>\n",
       "      <td>0.745968</td>\n",
       "      <td>0.745972</td>\n",
       "      <td>0.746387</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>1.828523</td>\n",
       "      <td>0.762097</td>\n",
       "      <td>0.762285</td>\n",
       "      <td>0.763294</td>\n",
       "      <td>0.762097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>1.942820</td>\n",
       "      <td>0.756720</td>\n",
       "      <td>0.756877</td>\n",
       "      <td>0.757107</td>\n",
       "      <td>0.756720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>1.953308</td>\n",
       "      <td>0.759409</td>\n",
       "      <td>0.759795</td>\n",
       "      <td>0.762922</td>\n",
       "      <td>0.759409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>1.976434</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>0.754522</td>\n",
       "      <td>0.758443</td>\n",
       "      <td>0.753360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.889008</td>\n",
       "      <td>0.755376</td>\n",
       "      <td>0.753680</td>\n",
       "      <td>0.755387</td>\n",
       "      <td>0.755376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.938413</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.757681</td>\n",
       "      <td>0.764661</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>1.848421</td>\n",
       "      <td>0.775538</td>\n",
       "      <td>0.775303</td>\n",
       "      <td>0.775412</td>\n",
       "      <td>0.775538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.941579</td>\n",
       "      <td>0.756720</td>\n",
       "      <td>0.756076</td>\n",
       "      <td>0.758608</td>\n",
       "      <td>0.756720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.965487</td>\n",
       "      <td>0.754704</td>\n",
       "      <td>0.755697</td>\n",
       "      <td>0.760225</td>\n",
       "      <td>0.754704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>1.900889</td>\n",
       "      <td>0.765457</td>\n",
       "      <td>0.766787</td>\n",
       "      <td>0.770716</td>\n",
       "      <td>0.765457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>1.892345</td>\n",
       "      <td>0.774866</td>\n",
       "      <td>0.774145</td>\n",
       "      <td>0.774427</td>\n",
       "      <td>0.774866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>1.911134</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.772372</td>\n",
       "      <td>0.776085</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>1.937653</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>0.775275</td>\n",
       "      <td>0.781443</td>\n",
       "      <td>0.776882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.975113</td>\n",
       "      <td>0.769489</td>\n",
       "      <td>0.770021</td>\n",
       "      <td>0.774236</td>\n",
       "      <td>0.769489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.907779</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.769443</td>\n",
       "      <td>0.770558</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>1.910553</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.783171</td>\n",
       "      <td>0.783338</td>\n",
       "      <td>0.783602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>1.918241</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.777584</td>\n",
       "      <td>0.780779</td>\n",
       "      <td>0.778898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>1.954546</td>\n",
       "      <td>0.771505</td>\n",
       "      <td>0.770335</td>\n",
       "      <td>0.771234</td>\n",
       "      <td>0.771505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.966388</td>\n",
       "      <td>0.768145</td>\n",
       "      <td>0.767759</td>\n",
       "      <td>0.767550</td>\n",
       "      <td>0.768145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.953016</td>\n",
       "      <td>0.773522</td>\n",
       "      <td>0.770881</td>\n",
       "      <td>0.777310</td>\n",
       "      <td>0.773522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>2.105201</td>\n",
       "      <td>0.755376</td>\n",
       "      <td>0.756606</td>\n",
       "      <td>0.760316</td>\n",
       "      <td>0.755376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1.933426</td>\n",
       "      <td>0.775538</td>\n",
       "      <td>0.773286</td>\n",
       "      <td>0.776074</td>\n",
       "      <td>0.775538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.924369</td>\n",
       "      <td>0.782930</td>\n",
       "      <td>0.782789</td>\n",
       "      <td>0.784981</td>\n",
       "      <td>0.782930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.907105</td>\n",
       "      <td>0.782930</td>\n",
       "      <td>0.782472</td>\n",
       "      <td>0.782295</td>\n",
       "      <td>0.782930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.933050</td>\n",
       "      <td>0.780242</td>\n",
       "      <td>0.780008</td>\n",
       "      <td>0.780053</td>\n",
       "      <td>0.780242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.973383</td>\n",
       "      <td>0.773522</td>\n",
       "      <td>0.773624</td>\n",
       "      <td>0.774083</td>\n",
       "      <td>0.773522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-11-763863ad85b1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14905, training_loss=0.16941646229137075, metrics={'train_runtime': 7316.9973, 'train_samples_per_second': 48.807, 'train_steps_per_second': 2.222, 'total_flos': 2.153320210685952e+16, 'train_loss': 0.16941646229137075, 'epoch': 55.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset (assuming CSV format with columns 'text' and 'label')\n",
    "data = df.copy()\n",
    "texts = data['Reviews'].tolist()\n",
    "labels = data['labels'].tolist()\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the mBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def polyglot_tokenize(text):\n",
    "    polyglot_text = Text(text, hint_language_code='hi')  # Tokenize with Polyglot\n",
    "    return polyglot_text.words  # Return tokens as a list\n",
    "\n",
    "def custom_tokenize(texts, tokenizer, max_length=128):\n",
    "    all_input_ids = []\n",
    "    for text in texts:\n",
    "        # Step 1: Tokenize using Polyglot\n",
    "        polyglot_tokens = polyglot_tokenize(text)\n",
    "\n",
    "        # Step 2: Map Polyglot tokens to mBERT vocabulary\n",
    "        input_ids = []\n",
    "        for token in polyglot_tokens:\n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            if token_id is None:  # Handle tokens not in mBERT's vocabulary\n",
    "                token_id = tokenizer.unk_token_id  # Use [UNK] token ID\n",
    "            input_ids.append(token_id)\n",
    "\n",
    "        # Step 3: Truncate sequences if necessary\n",
    "        if len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "\n",
    "    # Step 4: Pad sequences to the maximum length\n",
    "    encoded = tokenizer.pad(\n",
    "        {\"input_ids\": all_input_ids},\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded\n",
    "\n",
    "\n",
    "train_encodings = custom_tokenize(train_texts, tokenizer)\n",
    "test_encodings = custom_tokenize(test_texts, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2} # Create a mapping\n",
    "train_labels = [label_mapping[label] for label in train_labels] # Apply mapping\n",
    "test_labels = [label_mapping[label] for label in test_labels]\n",
    "\n",
    "\n",
    "# Create Dataset objects for Trainer\n",
    "class HindiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = HindiTextDataset(train_encodings, train_labels)\n",
    "test_dataset = HindiTextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Load the mBERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(set(labels)))\n",
    "model.to(device)\n",
    "\n",
    "# Define multiple metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Set up Trainer with early stopping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=22,\n",
    "    per_device_eval_batch_size=22,\n",
    "    num_train_epochs=60,  # Max epochs\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=2,  # Limit saved checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,  # Evaluate on test dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a9009bde3882>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation Results: {'eval_loss': 1.1041765213012695, 'eval_accuracy': 0.8649193548387096, 'eval_f1': 0.8646433553096033, 'eval_precision': 0.8649455810283394, 'eval_recall': 0.8649193548387096, 'eval_runtime': 11.009, 'eval_samples_per_second': 135.162, 'eval_steps_per_second': 6.177, 'epoch': 30.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Evaluation Results:\", test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
